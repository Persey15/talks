-------------
Size of data:
-------------

100-24:         196K
10,000-24:       196K
1,000,000-24:     1.8M
10,000,000-24:    16M
10,000,000-96:    17M
100,000,000-24:   171M
100,000,000-48:   189M
100,000,000-96:   198M
100,000,000-192:  169M
100,000,000-384:  165M
100,000,000-984:  169M
1,000,000,000-500: 3.3G


----------------------------
Partitioning on 100,000,000:
----------------------------

100k, 984 files, 41 per hr
250k, 384 files, 16 per hr
500k, 192 files, 8 per hr
1m, 96 files, 4 per hr
2m, 48 files, 2 per hr *
4m, 24 files, 1 per hr

*: best for Spark and Dask

---------------------------------------
Dask with default scheduler and python3
---------------------------------------

(talks3) --- daskvsspark/daskvsspark ‹master*AM› » ./tmp_run_all_dask.sh                                                                                  1 ↵
10 records, 24 files (1.0 per hour): done in 0:00:04.280264.
100 records, 24 files (1.0 per hour): done in 0:00:01.490881.
10,000 records, 24 files (1.0 per hour): done in 0:00:02.811427.
1,000,000 records, 24 files (1.0 per hour): done in 0:00:03.013248.
10,000,000 records, 24 files (1.0 per hour): done in 0:00:06.194535.
10,000,000 records, 96 files (4.0 per hour): done in 0:00:08.708831.
100,000,000 records, 192 files (8.0 per hour): done in 0:00:41.575053.
100,000,000 records, 24 files (1.0 per hour): done in 0:01:03.122334.
100,000,000 records, 384 files (16.0 per hour): done in 0:00:49.119739.
100,000,000 records, 48 files (2.0 per hour): done in 0:00:37.713205. *
100,000,000 records, 96 files (4.0 per hour): done in 0:00:38.806466.
100,000,000 records, 984 files (41.0 per hour): done in 0:01:10.351981.
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:16:11.660423.

---------------------------------
Spark with python3 and custom agg
---------------------------------

(talks3) --- daskvsspark/daskvsspark ‹master*M› » ./tmp_run_all_spark.sh                                                                                126 ↵
10 records, 24 files (1.0 per hour): done in 0:00:10.601392.
100 records, 24 files (1.0 per hour): done in 0:00:11.315226.
10,000 records, 24 files (1.0 per hour): done in 0:00:11.744349.
1,000,000 records, 24 files (1.0 per hour): done in 0:00:15.394712.
10,000,000 records, 24 files (1.0 per hour): done in 0:00:29.044079.
10,000,000 records, 96 files (4.0 per hour): done in 0:00:34.295349.
100,000,000 records, 192 files (8.0 per hour): done in 0:02:52.175157.
100,000,000 records, 24 files (1.0 per hour): done in 0:02:57.805231.
100,000,000 records, 384 files (16.0 per hour): done in 0:03:14.743094.
100,000,000 records, 48 files (2.0 per hour): done in 0:02:50.821578. *
100,000,000 records, 96 files (4.0 per hour): done in 0:03:09.673154.
100,000,000 records, 984 files (41.0 per hour): done in 0:03:41.323534.
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:45:08.288687.

