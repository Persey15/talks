-------------
Size of data:
-------------

100-24:         196K
10,000-24:       196K
1,000,000-24:     1.8M
10,000,000-24:    16M
10,000,000-96:    17M
100,000,000-24:   171M
100,000,000-48:   189M
100,000,000-96:   198M
100,000,000-192:  169M
100,000,000-384:  165M
100,000,000-984:  169M
1,000,000,000-500: 3.3G


----------------------------
Partitioning on 100,000,000:
----------------------------

100k, 984 files, 41 per hr
250k, 384 files, 16 per hr
500k, 192 files, 8 per hr
1m, 96 files, 4 per hr
2m, 48 files, 2 per hr *
4m, 24 files, 1 per hr

*: best for Spark and Dask

---------------------------------------
Dask with default scheduler and python3
---------------------------------------

(talks3) --- daskvsspark/daskvsspark ‹master*AM› » ./tmp_run_all_dask.sh                                                                                  1 ↵
10 records, 24 files (1.0 per hour): done in 0:00:04.280264.
100 records, 24 files (1.0 per hour): done in 0:00:01.490881.
10,000 records, 24 files (1.0 per hour): done in 0:00:02.811427.
1,000,000 records, 24 files (1.0 per hour): done in 0:00:03.013248.
10,000,000 records, 24 files (1.0 per hour): done in 0:00:06.194535.
10,000,000 records, 96 files (4.0 per hour): done in 0:00:08.708831.
100,000,000 records, 192 files (8.0 per hour): done in 0:00:41.575053.
100,000,000 records, 24 files (1.0 per hour): done in 0:01:03.122334.
100,000,000 records, 384 files (16.0 per hour): done in 0:00:49.119739.
100,000,000 records, 48 files (2.0 per hour): done in 0:00:37.713205. *
100,000,000 records, 96 files (4.0 per hour): done in 0:00:38.806466.
100,000,000 records, 984 files (41.0 per hour): done in 0:01:10.351981.
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:16:11.660423.

---------------------------------
Spark with python3 and custom agg
---------------------------------

(talks3) --- daskvsspark/daskvsspark ‹master*M› » ./tmp_run_all_spark.sh                                                                                126 ↵
10 records, 24 files (1.0 per hour): done in 0:00:10.601392.
100 records, 24 files (1.0 per hour): done in 0:00:11.315226.
10,000 records, 24 files (1.0 per hour): done in 0:00:11.744349.
1,000,000 records, 24 files (1.0 per hour): done in 0:00:15.394712.
10,000,000 records, 24 files (1.0 per hour): done in 0:00:29.044079.
10,000,000 records, 96 files (4.0 per hour): done in 0:00:34.295349.
100,000,000 records, 192 files (8.0 per hour): done in 0:02:52.175157.
100,000,000 records, 24 files (1.0 per hour): done in 0:02:57.805231.
100,000,000 records, 384 files (16.0 per hour): done in 0:03:14.743094.
100,000,000 records, 48 files (2.0 per hour): done in 0:02:50.821578. *
100,000,000 records, 96 files (4.0 per hour): done in 0:03:09.673154.
100,000,000 records, 984 files (41.0 per hour): done in 0:03:41.323534.
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:45:08.288687.

---------------
Running on YARN
---------------

Master:
-------
m4.xlarge
8 vCore, 16 GiB memory, EBS only storage
EBS Storage:16 GiB

Core: 2
-------
c4.2xlarge
8 vCore, 15 GiB memory, EBS only storage
EBS Storage:16 GiB

Settings used:

    --driver-memory 8g
    --executor-memory 3g
    --num-executors 5
    --executor-cores 4
    --conf "spark.yarn.executor.memoryOverhead=2g"

In Dask, this would correspond to:

Master:
    $ dask-scheduler
    $ PYTHONPATH=/home/hadoop/reqs/daskvsspark-0.1-py3.6.egg dask-worker --nprocs 4 --memory-limit 2560M tcp://10.21.0.229:8786

    or

    $ PYTHONPATH=/home/hadoop/reqs/daskvsspark-0.1-py3.6.egg dask-worker --nthreads 4 --memory-limit 5G tcp://10.21.0.229:8786

Core (2):
    $ PYTHONPATH=/home/hadoop/reqs/daskvsspark-0.1-py3.6.egg dask-worker --nprocs 8 --memory-limit 2560M tcp://10.21.0.229:8786

    or

    $ PYTHONPATH=/home/hadoop/reqs/daskvsspark-0.1-py3.6.egg dask-worker --nprocs 2 --nthreads 4 --memory-limit 5G tcp://10.21.0.229:8786

That needs:

    $ export PATH="/home/hadoop/conda/bin:$PATH"
    $ source activate dvss


-----
Spark
-----

[hadoop@ip-10-21-0-173 daskvsspark]$ ./aggregate_spark_yarn.sh 1000000000 500
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:12:15.945298.
[hadoop@ip-10-21-0-173 daskvsspark]$ ./aggregate_spark_yarn.sh 1000000000 500
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:11:59.845888.
[hadoop@ip-10-21-0-173 daskvsspark]$ ./aggregate_spark_yarn.sh 1000000000 500
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:12:14.694722.

----------
Dask Yarn:
----------

n_workers=3, memory=5*1024, cpus=5
(dvss) [hadoop@ip-10-21-0-219 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.219:38933 1000000000 500
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:21:01.037516.

n_workers=5, memory=4548, cpus=3
(dvss) [hadoop@ip-10-21-0-219 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.219:35385 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:20:45.988793.

-------------
Dask console:
-------------

(dvss) [hadoop@ip-10-21-0-229 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.229:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:09:44.437957.
(dvss) [hadoop@ip-10-21-0-229 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.229:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:09:45.010486.
(dvss) [hadoop@ip-10-21-0-229 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.229:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:09:40.223227.
