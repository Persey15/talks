-------------
Size of data:
-------------

100-24:         196K
10,000-24:       196K
1,000,000-24:     1.8M
10,000,000-24:    16M
10,000,000-96:    17M
100,000,000-24:   171M
100,000,000-48:   189M
100,000,000-96:   198M
100,000,000-192:  169M
100,000,000-384:  165M
100,000,000-984:  169M
1,000,000,000-500: 3.3G


----------------------------
Partitioning on 100,000,000:
----------------------------

100k, 984 files, 41 per hr
250k, 384 files, 16 per hr
500k, 192 files, 8 per hr
1m, 96 files, 4 per hr
2m, 48 files, 2 per hr *
4m, 24 files, 1 per hr

*: best for Spark and Dask

---------------------------------------
Dask with default scheduler and python3
---------------------------------------

(talks3) --- daskvsspark/daskvsspark ‹master*AM› » ./tmp_run_all_dask.sh                                                                                  1 ↵
10 records, 24 files (1.0 per hour): done in 0:00:04.280264.
100 records, 24 files (1.0 per hour): done in 0:00:01.490881.
10,000 records, 24 files (1.0 per hour): done in 0:00:02.811427.
1,000,000 records, 24 files (1.0 per hour): done in 0:00:03.013248.
10,000,000 records, 24 files (1.0 per hour): done in 0:00:06.194535.
10,000,000 records, 96 files (4.0 per hour): done in 0:00:08.708831.
100,000,000 records, 192 files (8.0 per hour): done in 0:00:41.575053.
100,000,000 records, 24 files (1.0 per hour): done in 0:01:03.122334.
100,000,000 records, 384 files (16.0 per hour): done in 0:00:49.119739.
100,000,000 records, 48 files (2.0 per hour): done in 0:00:37.713205. *
100,000,000 records, 96 files (4.0 per hour): done in 0:00:38.806466.
100,000,000 records, 984 files (41.0 per hour): done in 0:01:10.351981.
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:16:11.660423.

---------------------------------
Spark with python3 and custom agg
---------------------------------

(talks3) --- daskvsspark/daskvsspark ‹master*M› » ./tmp_run_all_spark.sh                                                                                126 ↵
10 records, 24 files (1.0 per hour): done in 0:00:10.601392.
100 records, 24 files (1.0 per hour): done in 0:00:11.315226.
10,000 records, 24 files (1.0 per hour): done in 0:00:11.744349.
1,000,000 records, 24 files (1.0 per hour): done in 0:00:15.394712.
10,000,000 records, 24 files (1.0 per hour): done in 0:00:29.044079.
10,000,000 records, 96 files (4.0 per hour): done in 0:00:34.295349.
100,000,000 records, 192 files (8.0 per hour): done in 0:02:52.175157.
100,000,000 records, 24 files (1.0 per hour): done in 0:02:57.805231.
100,000,000 records, 384 files (16.0 per hour): done in 0:03:14.743094.
100,000,000 records, 48 files (2.0 per hour): done in 0:02:50.821578. *
100,000,000 records, 96 files (4.0 per hour): done in 0:03:09.673154.
100,000,000 records, 984 files (41.0 per hour): done in 0:03:41.323534.
1,000,000,000 records, 500 files (20.833333333333332 per hour): done in 0:45:08.288687.

---------------
Running on YARN
---------------

Master:
-------
m4.xlarge
8 vCore, 16 GiB memory, EBS only storage
EBS Storage:16 GiB

Core: 2
-------
c4.2xlarge
8 vCore, 15 GiB memory, EBS only storage
EBS Storage:16 GiB

Settings used:

    --driver-memory 8g
    --executor-memory 3g
    --num-executors 4
    --executor-cores 4
    --conf "spark.yarn.executor.memoryOverhead=2g"

In Dask, this would correspond to:

Master:
    $ dask-scheduler
    $ PYTHONPATH=/home/hadoop/reqs/daskvsspark-0.1-py3.6.egg dask-worker --nthreads 4 --memory-limit 5G tcp://10.21.0.76:8786

Core (2):
    $ PYTHONPATH=/home/hadoop/reqs/daskvsspark-0.1-py3.6.egg dask-worker --nprocs 2 --nthreads 4 --memory-limit 5G tcp://10.21.0.76:8786


To run `aggregate_dask.sh`:

    $ export PATH="/home/hadoop/conda/bin:$PATH"
    $ source activate dvss


-------------
YARN REST API
-------------

curl -s $HOSTNAME:8088/ws/v1/cluster | jq
curl -s $HOSTNAME:8088/ws/v1/cluster/metrics | jq
curl -s $HOSTNAME:8088/ws/v1/cluster/scheduler | jq

    "totalMB": 23040,
    "totalVirtualCores": 16,
    "totalNodes": 2,

-----
Spark
-----

[hadoop@ip-10-21-0-173 daskvsspark]$ ./aggregate_spark_yarn.sh 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:12:15.945298.
[hadoop@ip-10-21-0-173 daskvsspark]$ ./aggregate_spark_yarn.sh 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:11:59.845888.
[hadoop@ip-10-21-0-173 daskvsspark]$ ./aggregate_spark_yarn.sh 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:12:14.694722.

----------
Dask Yarn:
----------

n_workers=3, memory=5*1024, cpus=5: uses 15/16 cpus, but only 15/20 mem
[hadoop@ip-10-21-0-76 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.76:40795 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:13:59.890983.

n_workers=4, memory=5*1024, cpus=3: only uses 12 cpus, but all the mem
[hadoop@ip-10-21-0-76 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.76:36955 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:11:20.381808.

n_workers=5, memory=3*1024, cpus=3: uses 15/16 cpus, but only 15/20 mem
[hadoop@ip-10-21-0-76 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.76:38815 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:11:21.899824.

* few workers: not good
* small workers: not good

-------------
Dask console:
-------------

5 workers (2 x core + 1 x master) with 4 cores and 5G memory each:

(dvss) [hadoop@ip-10-21-0-229 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.229:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:09:44.437957.
(dvss) [hadoop@ip-10-21-0-229 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.229:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:09:45.010486.
(dvss) [hadoop@ip-10-21-0-229 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.229:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:09:40.223227.

4 workers (2 x core) with 4 cores and 5G memory each:

(dvss) [hadoop@ip-10-21-0-76 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.76:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:11:29.925453.
(dvss) [hadoop@ip-10-21-0-76 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.76:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:11:37.121077.
(dvss) [hadoop@ip-10-21-0-76 daskvsspark]$ ./aggregate_dask_yarn.sh tcp://10.21.0.76:8786 1000000000 500
1,000,000,000 records, 500 files (20 per hour): done in 0:11:43.173057.
